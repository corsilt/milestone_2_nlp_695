{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "processing_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-GF4DOX9qVa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2KWfYTlr9zlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('WikiLarge_Test.csv') # will be test when run it\n",
        "concreteness = pd.read_csv(r'C:\\Users\\corsi\\Downloads\\umich-siads-695-fall21-predicting-text-difficulty\\Concreteness_ratings_Brysbaert_et_al_BRM.txt', delimiter = \"\\t\")\n",
        "basic = pd.read_csv(r'C:\\Users\\corsi\\Downloads\\umich-siads-695-fall21-predicting-text-difficulty\\dale_chall.txt', delimiter = \"\\t\", header=None)\n",
        "aol =pd.read_csv(r'C:\\Users\\corsi\\Downloads\\umich-siads-695-fall21-predicting-text-difficulty\\AoA_51715_words.csv', header=0,encoding = \"ISO-8859-1\" )"
      ],
      "metadata": {
        "id": "CvMsxl0L9EW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_lg\n",
        "nlp_model = en_core_web_lg.load()\n",
        "from spacy.tokenizer import Tokenizer\n",
        "tokenizer = Tokenizer(nlp_model.vocab)"
      ],
      "metadata": {
        "id": "1dUIZsHRf--a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text_no_punc(text): #lemma_word1.append(token.lemma_)\n",
        "    text = \" \".join([w.lower() for w in text.split()]) # may not need it's lowercasing beforing sending to tokenizer appears to make some difference like References \n",
        "    return \" \".join([w.lemma_.lower() for w in tokenizer(text) if nlp_model.vocab[w.lemma_].is_punct == False])\n",
        "\n",
        "def lemmatize_text(text): #lemma_word1.append(token.lemma_)\n",
        "    text = \" \".join([w.lower() for w in text.split()]) # may not need it's lowercasing beforing sending to tokenizer appears to make some difference like References \n",
        "    return \" \".join([w.lemma_.lower() for w in tokenizer(text)])"
      ],
      "metadata": {
        "id": "ExxRRTtr0hMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic.rename(columns={0:'Word'},inplace=True)\n",
        "basic['Word'] = basic['Word'].astype(str)\n",
        "basic['Word_lem'] = basic['Word'].apply(lemmatize_text)\n",
        "\n",
        "concreteness['Word'] = concreteness['Word'].astype(str)\n",
        "concreteness['Word_lem'] = concreteness['Word'].apply(lemmatize_text)\n",
        "\n",
        "aol['Word'] = aol['Word'].astype(str)\n",
        "aol['Word_lem'] = aol['Word'].apply(lemmatize_text)\n",
        "\n",
        "df['text_lemma'] = df['original_text'].apply(lemmatize_text)\n",
        "df['text_lemma_no_punc'] = df['original_text'].apply(lemmatize_text_no_punc)"
      ],
      "metadata": {
        "id": "KD6K69kiZ1Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_word_set = set(basic['Word_lem'].to_list())\n",
        "\n",
        "def basic_rank(text): # could also potentially reduce to stem to check as well stampeders should map to stamp\n",
        "  split_sent = text.split()\n",
        "  split_len = len(split_sent)\n",
        "  count = 0\n",
        "  count = len(basic_word_set.intersection(set(split_sent)))\n",
        "  # for i in split_sent:\n",
        "  #   if len(basic_word_set.intersection(i)) == 1:\n",
        "  #     count+=1\n",
        "  try:\n",
        "    return count/split_len\n",
        "  except:\n",
        "    return 0\n",
        "\n",
        "def oov(text): # higher less words oov\n",
        "  split_sent = text.split()\n",
        "  split_len = len(split_sent)\n",
        "  count = 0\n",
        "  for i in split_sent:\n",
        "    if nlp_model.vocab[i].is_oov == False:\n",
        "      count+=1\n",
        "  \n",
        "  try:\n",
        "    return count/split_len\n",
        "  except:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "LhWxepq7c4hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['basic_rank'] = df['text_lemma'].apply(basic_rank)\n",
        "df['basic_rank_no_punc'] = df['text_lemma_no_punc'].apply(basic_rank) # could be more accurate because not diluted by punctuation\n",
        "\n",
        "df['oov_rank'] = df['text_lemma'].apply(oov)\n",
        "df['oov_rank_no_punc'] = df['text_lemma_no_punc'].apply(oov)"
      ],
      "metadata": {
        "id": "p5z5YoF3lGEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "d = defaultdict(float, {'a':3.23423})"
      ],
      "metadata": {
        "id": "LjzC8IgQmkRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concreteness_dict = dict(zip(concreteness['Word_lem'],concreteness['Conc.M'] )) # should also try scaling by perc known can multiple right here when creating initial dict\n",
        "concreteness_dict_perc = dict(zip(concreteness['Word_lem'],concreteness['Conc.M']*concreteness['Percent_known'] )) # should also try scaling by perc known can multiple right here when creating initial dict\n",
        "def concreteness_rank(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, concreteness_dict)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "def concreteness_rank_nan(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, concreteness_dict)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "  text_array[text_array == 0.0] = np.nan\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "def concreteness_rank_perc(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, concreteness_dict_perc)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "def concreteness_rank_nan_perc(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, concreteness_dict_perc)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "  text_array[text_array == 0.0] = np.nan\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n"
      ],
      "metadata": {
        "id": "6osHzRQ3orfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aol_dict = dict(zip(aol['Word_lem'],aol['AoA_Kup_lem'] )) \n",
        "def aol_rank(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, aol_dict)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "\n",
        "def aol_rank_nan(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, aol_dict)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "  text_array[text_array == 0.0] = np.nan\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "\n",
        "aol_dict_perc = dict(zip(aol['Word_lem'],aol['AoA_Kup_lem']*aol['Perc_known'] )) \n",
        "def aol_rank_perc(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, aol_dict_perc)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "\n",
        "def aol_rank_nan_perc(text):\n",
        "  import numpy as np\n",
        "  from collections import defaultdict\n",
        "  temp_dict = defaultdict(float, aol_dict_perc)\n",
        "  split_sent = text.split()\n",
        "\n",
        "  text_array = np.array([temp_dict[i] for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "  text_array[text_array == 0.0] = np.nan\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array"
      ],
      "metadata": {
        "id": "r-2K2Cwzv0eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_length(text):\n",
        "  return len(text)\n",
        "\n",
        "def word_count(text):\n",
        "  return len(text.split())\n",
        "\n",
        "def word_length(text): #original text\n",
        "\n",
        "  import numpy as np\n",
        "  split_sent = [w for w in tokenizer(text) if nlp_model.vocab[w.lemma_].is_punct == False]\n",
        "  \n",
        "\n",
        "  text_array = np.array([len(i) for i in split_sent]) # if not present substitute 0 should think about making nan ht = defaultdict(lambda:np.Nan, ht), arr[arr == 0] = 'nan' # or use np.nan\n",
        "  average_array = np.nanmean(text_array)\n",
        "\n",
        "\n",
        "  return average_array\n",
        "\n",
        "def stop_word(text): #original text\n",
        "  split_sent = [w for w in tokenizer(text) if nlp_model.vocab[w.lemma_].is_stop == True]\n",
        "  return len(split_sent)\n",
        "\n",
        "\n",
        "def numbers(text): #original text\n",
        "  split_sent = [w for w in tokenizer(text) if nlp_model.vocab[w.lemma_].is_digit == True]\n",
        "  return len(split_sent)\n",
        "\n",
        "\n",
        "\n",
        "def rare_punctuation(text):\n",
        "  import string \n",
        "      \n",
        "  # Storing the sets of punctuation in variable result \n",
        "  result = '''!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ #'''# removed quotes and .\n",
        "  \n",
        "  split_sent = text.split()\n",
        "  count = 0\n",
        "  for i in split_sent:\n",
        "    if i in result:\n",
        "      count +=1\n",
        "\n",
        "  return count\n",
        "\n"
      ],
      "metadata": {
        "id": "tMM7arjh27we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_len'] = df['original_text'].apply(word_length) #lemmatize will shorten only tokenize\n",
        "\n",
        "\n",
        "df['stop_word'] = df['original_text'].apply(stop_word)\n",
        "df['nums'] = df['original_text'].apply(numbers)\n",
        "\n",
        "df['word_len_no_punc'] = df['text_lemma_no_punc'].apply(word_length)\n",
        "\n",
        "df['sent_len'] = df['original_text'].apply(sent_length)\n",
        "\n",
        "df['word_count_no_punct'] = df['text_lemma_no_punc'].apply(word_count)\n",
        "df['word_count'] = df['text_lemma'].apply(word_count)\n",
        "\n",
        "\n",
        "df['punct'] = df['original_text'].apply(rare_punctuation)\n",
        "df['punct_lemm'] = df['text_lemma'].apply(rare_punctuation)"
      ],
      "metadata": {
        "id": "Dj730kW827wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e6a60b-f939-455d-c69f-2bb68fbf9302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/1180071782.py:14: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['concreteness_rank_nan'] = df['text_lemma'].apply(concreteness_rank_nan)\n",
        "df['concreteness_rank'] = df['text_lemma'].apply(concreteness_rank)\n",
        "\n",
        "df['concreteness_rank_nan_no_punc'] = df['text_lemma_no_punc'].apply(concreteness_rank_nan)\n",
        "df['concreteness_rank_no_punc'] = df['text_lemma_no_punc'].apply(concreteness_rank)\n",
        "\n",
        "df['concreteness_rank_nan_perc'] = df['text_lemma'].apply(concreteness_rank_nan_perc)\n",
        "df['concreteness_rank_perc'] = df['text_lemma'].apply(concreteness_rank_perc)\n",
        "\n",
        "df['concreteness_rank_nan_no_punc_perc'] = df['text_lemma_no_punc'].apply(concreteness_rank_nan_perc)\n",
        "df['concreteness_rank_no_punc_perc'] = df['text_lemma_no_punc'].apply(concreteness_rank_perc)\n",
        "\n",
        "\n",
        "\n",
        "df['aol_rank_nan'] = df['text_lemma'].apply(aol_rank_nan)\n",
        "df['aol_rank'] = df['text_lemma'].apply(aol_rank)\n",
        "\n",
        "df['aol_rank_nan_no_punc'] = df['text_lemma_no_punc'].apply(aol_rank_nan)\n",
        "df['aol_rank_no_punc'] = df['text_lemma_no_punc'].apply(aol_rank)\n",
        "\n",
        "\n",
        "df['aol_rank_nan_perc'] = df['text_lemma'].apply(aol_rank_nan_perc)\n",
        "df['aol_rank_perc'] = df['text_lemma'].apply(aol_rank_perc)\n",
        "\n",
        "df['aol_rank_nan_no_punc_perc'] = df['text_lemma_no_punc'].apply(aol_rank_nan_perc)\n",
        "df['aol_rank_no_punc_perc'] = df['text_lemma_no_punc'].apply(aol_rank_perc)"
      ],
      "metadata": {
        "id": "OOj6QnbXsiXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04131c6b-6067-4dc5-f320-2584f3f96fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/1460970154.py:24: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n",
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/1460970154.py:11: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n",
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/1460970154.py:50: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n",
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/1460970154.py:37: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n",
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/3577557391.py:10: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n",
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/3577557391.py:53: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n",
            "C:\\Users\\corsi\\AppData\\Local\\Temp/ipykernel_3696/3577557391.py:39: RuntimeWarning: Mean of empty slice\n",
            "  average_array = np.nanmean(text_array)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_lemma_no_punc'].isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVuAbtTIQtY0",
        "outputId": "eee1a8b9-f2c3-47a1-b80b-5d1557aa317a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# YOUR CODE HERE\n",
        "first_pass = []\n",
        "tokenized_train_items = []\n",
        "skip = set(stopwords.words('english'))\n",
        "\n",
        "for bio in tqdm(df['text_lemma_no_punc']):\n",
        "  try:\n",
        "    temp = list(re.findall(r'\\w+', bio))\n",
        "    first_pass.append(temp)\n",
        "\n",
        "    temp = [w.lower() for w in temp if w.lower() not in skip]\n",
        "    tokenized_train_items.append(temp)\n",
        "  except:\n",
        "    tokenized_train_items.append('lemmanansnotext')"
      ],
      "metadata": {
        "id": "ChNlw5kCIQRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed0bf09-9937-4170-9317-bbeda635fc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\corsi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 119092/119092 [00:02<00:00, 52875.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"D:\\Downloads\\glove.6b\\glove.6B.300d.txt\", binary=False)"
      ],
      "metadata": {
        "id": "-elOOi2EI9kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dense_features(tokenized_texts, word_vectors): \n",
        "    #HINT: Create an empty list to hold your results \n",
        "        #HINT:Iterate through each item in tokenized_text\n",
        "            #HINT:Create a list that contains current item(s) if found in word_vectors.vocab\n",
        "            #HINT:if the length of this list is greater than zero:\n",
        "                #HINT:We set this as a feature, this is done by using numpy’s mean function and append it to our results list \n",
        "            #HINT:Otherwise: create a vector of numpy zeros using word_vectors.vector_size as the parameter and append it to the results list\n",
        "    #HINT:Return the results list as a numpy array (data type)\n",
        "    import numpy as np\n",
        "    # YOUR CODE HERE\n",
        "    results = []\n",
        "    for item in tokenized_texts:\n",
        "        temp = [i for i in item if i in word_vectors.vocab]\n",
        "        if len(temp) > 0 :\n",
        "            results.append(np.mean(word_vectors[temp],axis=0)) #can also pass all words in at once, but did this method\n",
        "            \n",
        "        else:\n",
        "            results.append(np.zeros(word_vectors.vector_size))\n",
        "            \n",
        "    return np.array(results)"
      ],
      "metadata": {
        "id": "zEYn4qeQOs14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_wp = generate_dense_features(tokenized_train_items, model)"
      ],
      "metadata": {
        "id": "w-KBKR-2R_kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shouldn't need this cell\n",
        "\n",
        "# first_pass = []\n",
        "# tokenized_train_items_dev = []\n",
        "# skip = set(stopwords.words('english'))\n",
        "\n",
        "# for bio in tqdm(dev_df['original_text']):\n",
        "#     temp = list(re.findall(r'\\w+', bio))\n",
        "#     first_pass.append(temp)\n",
        "\n",
        "#     temp = [w for w in temp if w.lower() not in skip]\n",
        "#     tokenized_train_items_dev.append(temp)\n",
        "\n",
        "# first_pass = []\n",
        "# tokenized_train_items_test = []\n",
        "# skip = set(stopwords.words('english'))\n",
        "\n",
        "# for bio in tqdm(test['original_text']):\n",
        "#     temp = list(re.findall(r'\\w+', bio))\n",
        "#     first_pass.append(temp)\n",
        "\n",
        "#     temp = [w for w in temp if w.lower() not in skip]\n",
        "#     tokenized_train_items_test.append(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POFU3EHnaYQK",
        "outputId": "2f3012ee-74fc-4edb-f2e5-1ddecd53db5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41677/41677 [00:01<00:00, 24228.73it/s]\n",
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41677/41677 [00:00<00:00, 58762.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_dev = generate_dense_features(tokenized_train_items_dev, model)\n",
        "# X_test= generate_dense_features(tokenized_train_items_test, model)"
      ],
      "metadata": {
        "id": "xr1fvq9AbWtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gYZiPiFyU2pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concreteness['scaled_m'] = concreteness['Conc.M']*concreteness['Percent_known']\n"
      ],
      "metadata": {
        "id": "Hv3S6UTIcTed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concreteness['quantiles'] = pd.qcut(concreteness['scaled_m'],labels=False, q=10)"
      ],
      "metadata": {
        "id": "1zcfQfiN86Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aol['scaled_m'] = aol['AoA_Kup_lem']*aol['Perc_known_lem']\n",
        "aol['quantiles'] = pd.qcut(aol['scaled_m'],labels=False, q=10)"
      ],
      "metadata": {
        "id": "gWONC_qhU3CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_quartiles_concreteness = concreteness[concreteness['quantiles'].isin([7,8,9])]['Word'].to_list()\n",
        "middle_quartiles_concreteness = concreteness[concreteness['quantiles'].isin([4,5,6])]['Word'].to_list()\n",
        "lower_quartiles_concreteness = concreteness[concreteness['quantiles'].isin([0,1,2,3])]['Word'].to_list()"
      ],
      "metadata": {
        "id": "hcWZ_vux-Hrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aol['scaled_m'] = aol['AoA_Kup_lem']*aol['Perc_known_lem']\n",
        "aol['quantiles'] = pd.qcut(aol['scaled_m'],labels=False, q=10)"
      ],
      "metadata": {
        "id": "dAu5j1ik_IYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_quartiles_aol = aol[aol['quantiles'].isin([7,8,9])]['Word'].to_list()\n",
        "middle_quartiles_aol = aol[aol['quantiles'].isin([4,5,6])]['Word'].to_list()\n",
        "lower_quartiles_aol = aol[aol['quantiles'].isin([0,1,2,3])]['Word'].to_list()"
      ],
      "metadata": {
        "id": "2VWKzJft_thO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quartile_similarity(tokenized_texts, quartile_list):\n",
        "\n",
        "  import numpy as np\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  top = [i for i in quartile_list[0] if i in model.vocab]\n",
        "  mid = [i for i in quartile_list[1] if i in model.vocab]\n",
        "  bottom = [i for i in quartile_list[2] if i in model.vocab]\n",
        "\n",
        "  top_vec = np.mean(model[top],axis=0)\n",
        "  mid_vec = np.mean(model[mid],axis=0)\n",
        "  bottom_vec = np.mean(model[bottom],axis=0)\n",
        "\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  top_results = []\n",
        "  mid_results = []\n",
        "  bottom_results = []\n",
        "\n",
        "  for item in tokenized_texts:\n",
        "      temp = [i for i in item if i in model.vocab]\n",
        "      if len(temp) > 0 :\n",
        "          temp_res = np.mean(model[temp],axis=0)\n",
        "          top_results.append(np.ravel(cosine_similarity(temp_res.reshape(-1,300,),top_vec.reshape(-1,300)))) \n",
        "          mid_results.append(np.ravel(cosine_similarity(temp_res.reshape(-1,300,),mid_vec.reshape(-1,300))))\n",
        "          bottom_results.append(np.ravel(cosine_similarity(temp_res.reshape(-1,300,),bottom_vec.reshape(-1,300))))\n",
        "          \n",
        "      else:\n",
        "          top_results.append(np.array([0.0]))\n",
        "          mid_results.append(np.array([0.0]))\n",
        "          bottom_results.append(np.array([0.0]))\n",
        "          \n",
        "  return top_results, mid_results, bottom_results\n"
      ],
      "metadata": {
        "id": "fJ2-G0O_BBlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_aol,mid_aol,bot_aol = quartile_similarity(tokenized_train_items, [top_quartiles_aol, middle_quartiles_aol, lower_quartiles_aol])"
      ],
      "metadata": {
        "id": "d7elurqmCfVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_con,mid_con,bot_con = quartile_similarity(tokenized_train_items, [top_quartiles_concreteness, middle_quartiles_concreteness, lower_quartiles_concreteness])"
      ],
      "metadata": {
        "id": "sTHCtBJjCgDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quartile_similarity_single(tokenized_texts, quartile_list):\n",
        "\n",
        "  import numpy as np\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  top = [i for i in quartile_list if i in model.vocab]\n",
        "\n",
        "  top_vec = np.mean(model[top],axis=0)\n",
        "\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  top_results = []\n",
        "  mid_results = []\n",
        "  bottom_results = []\n",
        "\n",
        "  for item in tokenized_texts:\n",
        "      temp = [i for i in item if i in model.vocab]\n",
        "      if len(temp) > 0 :\n",
        "          temp_res = np.mean(model[temp],axis=0)\n",
        "          top_results.append(np.ravel(cosine_similarity(temp_res.reshape(-1,300,),top_vec.reshape(-1,300)))) \n",
        "\n",
        "          \n",
        "      else:\n",
        "          top_results.append(np.array([0.0]))\n",
        "\n",
        "          \n",
        "  return top_results"
      ],
      "metadata": {
        "id": "MEmqXvPLK5eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_word = basic['Word'].to_list()\n",
        "basic_sim = quartile_similarity_single(tokenized_train_items, basic_word)"
      ],
      "metadata": {
        "id": "GkqqQpxHSkf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s =  ['top_aol','mid_aol','bot_aol','top_con','mid_con','bot_con','basic_sim']\n",
        "for i in s:\n",
        "  df[i] = eval(i)"
      ],
      "metadata": {
        "id": "QJp_WJmGQH-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# df = pd.read_csv(r'C:\\Users\\corsi\\Downloads\\umich-siads-695-fall21-predicting-text-difficulty\\test_set_processed.csv')"
      ],
      "metadata": {
        "id": "OE7xGcrAQIx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['aol_rank_nan_perc']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_20P5cWfwxN",
        "outputId": "c59012f4-8b8c-4b31-abad-ae1614730b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        NaN\n",
              "1        NaN\n",
              "2        NaN\n",
              "3        NaN\n",
              "4        NaN\n",
              "          ..\n",
              "119087   NaN\n",
              "119088   NaN\n",
              "119089   NaN\n",
              "119090   NaN\n",
              "119091   NaN\n",
              "Name: aol_rank_nan_perc, Length: 119092, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "objs = ['mid_con', 'bot_aol', 'mid_aol', 'top_aol', 'bot_con', 'top_con',\n",
        "       'basic_sim']\n",
        "\n",
        "def make_float(col):\n",
        "  return (col)[0]\n",
        "for i in objs:\n",
        "  df[i] = df[i].apply(make_float)\n",
        "for i in objs:\n",
        "  df[i] = pd.to_numeric(df[i], errors='coerce')\n"
      ],
      "metadata": {
        "id": "t2VEZ40hQHn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(r'C:\\Users\\corsi\\Downloads\\umich-siads-695-fall21-predicting-text-difficulty\\test_set_processed.csv')"
      ],
      "metadata": {
        "id": "eqy2cIH5QRCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['bas']"
      ],
      "metadata": {
        "id": "n2PAsi_sRM1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "ee7ee9df-0ef5-45b8-f7a0-a1d64ff3ec66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id original_text  label text_lemma text_lemma_no_punc  basic_rank  \\\n",
              "0            0         -2011    NaN      -2011              -2011         0.0   \n",
              "1            1         -2011    NaN      -2011              -2011         0.0   \n",
              "2            2         -2000    NaN      -2000              -2000         0.0   \n",
              "3            3         -1997    NaN      -1997              -1997         0.0   \n",
              "4            4         1.636    NaN      1.636              1.636         0.0   \n",
              "...        ...           ...    ...        ...                ...         ...   \n",
              "119087  119087        #NAME?    NaN     #name?             #name?         0.0   \n",
              "119088  119088        #NAME?    NaN     #name?             #name?         0.0   \n",
              "119089  119089        #NAME?    NaN     #name?             #name?         0.0   \n",
              "119090  119090        #NAME?    NaN     #name?             #name?         0.0   \n",
              "119091  119091        #NAME?    NaN     #name?             #name?         0.0   \n",
              "\n",
              "        basic_rank_no_punc  oov_rank  oov_rank_no_punc  word_len  ...  \\\n",
              "0                      0.0       1.0               1.0       5.0  ...   \n",
              "1                      0.0       1.0               1.0       5.0  ...   \n",
              "2                      0.0       1.0               1.0       5.0  ...   \n",
              "3                      0.0       0.0               0.0       5.0  ...   \n",
              "4                      0.0       0.0               0.0       5.0  ...   \n",
              "...                    ...       ...               ...       ...  ...   \n",
              "119087                 0.0       0.0               0.0       6.0  ...   \n",
              "119088                 0.0       0.0               0.0       6.0  ...   \n",
              "119089                 0.0       0.0               0.0       6.0  ...   \n",
              "119090                 0.0       0.0               0.0       6.0  ...   \n",
              "119091                 0.0       0.0               0.0       6.0  ...   \n",
              "\n",
              "        aol_rank_nan_perc  aol_rank_perc  aol_rank_nan_no_punc_perc  \\\n",
              "0                     NaN            0.0                        NaN   \n",
              "1                     NaN            0.0                        NaN   \n",
              "2                     NaN            0.0                        NaN   \n",
              "3                     NaN            0.0                        NaN   \n",
              "4                     NaN            0.0                        NaN   \n",
              "...                   ...            ...                        ...   \n",
              "119087                NaN            0.0                        NaN   \n",
              "119088                NaN            0.0                        NaN   \n",
              "119089                NaN            0.0                        NaN   \n",
              "119090                NaN            0.0                        NaN   \n",
              "119091                NaN            0.0                        NaN   \n",
              "\n",
              "        aol_rank_no_punc_perc        top_aol        mid_aol        bot_aol  \\\n",
              "0                         0.0   [-0.4457999]  [-0.45882088]  [-0.37928525]   \n",
              "1                         0.0   [-0.4457999]  [-0.45882088]  [-0.37928525]   \n",
              "2                         0.0    [-0.445449]  [-0.45983702]   [-0.3791357]   \n",
              "3                         0.0  [-0.45167747]   [-0.4718316]   [-0.3959525]   \n",
              "4                         0.0   [-0.2594516]   [-0.2654487]  [-0.19863749]   \n",
              "...                       ...            ...            ...            ...   \n",
              "119087                    0.0   [-0.3035713]  [-0.29423994]  [-0.14190874]   \n",
              "119088                    0.0   [-0.3035713]  [-0.29423994]  [-0.14190874]   \n",
              "119089                    0.0   [-0.3035713]  [-0.29423994]  [-0.14190874]   \n",
              "119090                    0.0   [-0.3035713]  [-0.29423994]  [-0.14190874]   \n",
              "119091                    0.0   [-0.3035713]  [-0.29423994]  [-0.14190874]   \n",
              "\n",
              "              top_con        mid_con        bot_con  \n",
              "0       [-0.28325474]   [-0.3743289]  [-0.41504705]  \n",
              "1       [-0.28325474]   [-0.3743289]  [-0.41504705]  \n",
              "2        [-0.2960482]  [-0.36524928]  [-0.40027258]  \n",
              "3       [-0.31601787]   [-0.3771605]  [-0.39861703]  \n",
              "4       [-0.09141784]  [-0.21138589]  [-0.29339764]  \n",
              "...               ...            ...            ...  \n",
              "119087  [-0.04912301]  [-0.20984508]  [-0.23277473]  \n",
              "119088  [-0.04912301]  [-0.20984508]  [-0.23277473]  \n",
              "119089  [-0.04912301]  [-0.20984508]  [-0.23277473]  \n",
              "119090  [-0.04912301]  [-0.20984508]  [-0.23277473]  \n",
              "119091  [-0.04912301]  [-0.20984508]  [-0.23277473]  \n",
              "\n",
              "[119092 rows x 40 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>original_text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_lemma</th>\n",
              "      <th>text_lemma_no_punc</th>\n",
              "      <th>basic_rank</th>\n",
              "      <th>basic_rank_no_punc</th>\n",
              "      <th>oov_rank</th>\n",
              "      <th>oov_rank_no_punc</th>\n",
              "      <th>word_len</th>\n",
              "      <th>...</th>\n",
              "      <th>aol_rank_nan_perc</th>\n",
              "      <th>aol_rank_perc</th>\n",
              "      <th>aol_rank_nan_no_punc_perc</th>\n",
              "      <th>aol_rank_no_punc_perc</th>\n",
              "      <th>top_aol</th>\n",
              "      <th>mid_aol</th>\n",
              "      <th>bot_aol</th>\n",
              "      <th>top_con</th>\n",
              "      <th>mid_con</th>\n",
              "      <th>bot_con</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2011</td>\n",
              "      <td>-2011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.4457999]</td>\n",
              "      <td>[-0.45882088]</td>\n",
              "      <td>[-0.37928525]</td>\n",
              "      <td>[-0.28325474]</td>\n",
              "      <td>[-0.3743289]</td>\n",
              "      <td>[-0.41504705]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2011</td>\n",
              "      <td>-2011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.4457999]</td>\n",
              "      <td>[-0.45882088]</td>\n",
              "      <td>[-0.37928525]</td>\n",
              "      <td>[-0.28325474]</td>\n",
              "      <td>[-0.3743289]</td>\n",
              "      <td>[-0.41504705]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-2000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2000</td>\n",
              "      <td>-2000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.445449]</td>\n",
              "      <td>[-0.45983702]</td>\n",
              "      <td>[-0.3791357]</td>\n",
              "      <td>[-0.2960482]</td>\n",
              "      <td>[-0.36524928]</td>\n",
              "      <td>[-0.40027258]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-1997</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1997</td>\n",
              "      <td>-1997</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.45167747]</td>\n",
              "      <td>[-0.4718316]</td>\n",
              "      <td>[-0.3959525]</td>\n",
              "      <td>[-0.31601787]</td>\n",
              "      <td>[-0.3771605]</td>\n",
              "      <td>[-0.39861703]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.636</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.636</td>\n",
              "      <td>1.636</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.2594516]</td>\n",
              "      <td>[-0.2654487]</td>\n",
              "      <td>[-0.19863749]</td>\n",
              "      <td>[-0.09141784]</td>\n",
              "      <td>[-0.21138589]</td>\n",
              "      <td>[-0.29339764]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119087</th>\n",
              "      <td>119087</td>\n",
              "      <td>#NAME?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#name?</td>\n",
              "      <td>#name?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.3035713]</td>\n",
              "      <td>[-0.29423994]</td>\n",
              "      <td>[-0.14190874]</td>\n",
              "      <td>[-0.04912301]</td>\n",
              "      <td>[-0.20984508]</td>\n",
              "      <td>[-0.23277473]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119088</th>\n",
              "      <td>119088</td>\n",
              "      <td>#NAME?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#name?</td>\n",
              "      <td>#name?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.3035713]</td>\n",
              "      <td>[-0.29423994]</td>\n",
              "      <td>[-0.14190874]</td>\n",
              "      <td>[-0.04912301]</td>\n",
              "      <td>[-0.20984508]</td>\n",
              "      <td>[-0.23277473]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119089</th>\n",
              "      <td>119089</td>\n",
              "      <td>#NAME?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#name?</td>\n",
              "      <td>#name?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.3035713]</td>\n",
              "      <td>[-0.29423994]</td>\n",
              "      <td>[-0.14190874]</td>\n",
              "      <td>[-0.04912301]</td>\n",
              "      <td>[-0.20984508]</td>\n",
              "      <td>[-0.23277473]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119090</th>\n",
              "      <td>119090</td>\n",
              "      <td>#NAME?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#name?</td>\n",
              "      <td>#name?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.3035713]</td>\n",
              "      <td>[-0.29423994]</td>\n",
              "      <td>[-0.14190874]</td>\n",
              "      <td>[-0.04912301]</td>\n",
              "      <td>[-0.20984508]</td>\n",
              "      <td>[-0.23277473]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119091</th>\n",
              "      <td>119091</td>\n",
              "      <td>#NAME?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#name?</td>\n",
              "      <td>#name?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[-0.3035713]</td>\n",
              "      <td>[-0.29423994]</td>\n",
              "      <td>[-0.14190874]</td>\n",
              "      <td>[-0.04912301]</td>\n",
              "      <td>[-0.20984508]</td>\n",
              "      <td>[-0.23277473]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>119092 rows × 40 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "id": "JEGblyxuhyJ4",
        "outputId": "9df51571-e39f-4f1e-c25c-ce6bffe7e41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0                                      original_text  label  \\\n",
              "0                0  There is manuscript evidence that Austen conti...      1   \n",
              "1                1  In a remarkable comparative analysis , Mandaea...      1   \n",
              "2                2  Before Persephone was released to Hermes , who...      1   \n",
              "3                3  Cogeneration plants are commonly found in dist...      1   \n",
              "4                4  Geneva -LRB- , ; , ; , ; ; -RRB- is the second...      1   \n",
              "...            ...                                                ...    ...   \n",
              "416763      416763  A Duke Nukem 3D version has been sold for Xbox...      0   \n",
              "416764      416764  However , it is becoming replaced as a method ...      0   \n",
              "416765      416765  There are hand gestures in both Hindu and Budd...      0   \n",
              "416766      416766  If it is necessary to use colors , try to choo...      0   \n",
              "416767      416767                               Calgary Stampeders ,      0   \n",
              "\n",
              "                                               text_lemma  \\\n",
              "0       there be manuscript evidence that austen conti...   \n",
              "1       in a remarkable comparative analysis , mandaea...   \n",
              "2       before persephone be release to hermes , who h...   \n",
              "3       cogeneration plant be commonly find in distric...   \n",
              "4       geneva -lrb- , ; , ; , ; ; -rrb- be the second...   \n",
              "...                                                   ...   \n",
              "416763  a duke nukem 3d version have be sell for xbox ...   \n",
              "416764  however , it be become replace a a method of e...   \n",
              "416765  there be hand gesture in both hindu and buddhi...   \n",
              "416766  if it be necessary to use color , try to choos...   \n",
              "416767                               calgary stampeders ,   \n",
              "\n",
              "                                       text_lemma_no_punc  basic_rank  \\\n",
              "0       there be manuscript evidence that austen conti...    0.348837   \n",
              "1       in a remarkable comparative analysis mandaean ...    0.304348   \n",
              "2       before persephone be release to hermes who hav...    0.478261   \n",
              "3       cogeneration plant be commonly find in distric...    0.435897   \n",
              "4       geneva -lrb- -rrb- be the second-most-populous...    0.250000   \n",
              "...                                                   ...         ...   \n",
              "416763  a duke nukem 3d version have be sell for xbox ...    0.470588   \n",
              "416764  however it be become replace a a method of exe...    0.550000   \n",
              "416765  there be hand gesture in both hindu and buddhi...    0.545455   \n",
              "416766  if it be necessary to use color try to choose ...    0.452381   \n",
              "416767                                 calgary stampeders    0.000000   \n",
              "\n",
              "        basic_rank_no_punc  oov_rank  oov_rank_no_punc  word_len  ...  \\\n",
              "0                 0.394737  1.000000          1.000000  4.394737  ...   \n",
              "1                 0.333333  0.869565          0.857143  6.190476  ...   \n",
              "2                 0.523810  0.956522          0.952381  4.738095  ...   \n",
              "3                 0.531250  1.000000          1.000000  6.281250  ...   \n",
              "4                 0.321429  0.777778          0.714286  5.642857  ...   \n",
              "...                    ...       ...               ...       ...  ...   \n",
              "416763            0.533333  1.000000          1.000000  4.066667  ...   \n",
              "416764            0.611111  1.000000          1.000000  5.000000  ...   \n",
              "416765            0.600000  1.000000          1.000000  5.300000  ...   \n",
              "416766            0.487179  0.952381          0.948718  4.410256  ...   \n",
              "416767            0.000000  1.000000          1.000000  8.500000  ...   \n",
              "\n",
              "        aol_rank_no_punc_perc  stop_word  nums   top_aol   mid_aol   bot_aol  \\\n",
              "0                    3.110604         17     3 -0.394263 -0.380458 -0.248316   \n",
              "1                    3.410769          7     0  0.026090 -0.024771 -0.064591   \n",
              "2                    3.921771         23     0 -0.227885 -0.169121  0.024210   \n",
              "3                    5.745808          6     0 -0.316036 -0.298752 -0.165115   \n",
              "4                    2.514700         12     0 -0.206032 -0.194760 -0.154996   \n",
              "...                       ...        ...   ...       ...       ...       ...   \n",
              "416763               1.926667          5     2 -0.367020 -0.345096 -0.209537   \n",
              "416764               5.725556         10     0 -0.191397 -0.221321 -0.186635   \n",
              "416765               5.422300          5     0  0.012151  0.023066  0.073097   \n",
              "416766               4.231075         18     0 -0.128926 -0.091671  0.108809   \n",
              "416767               0.000000          0     0 -0.072140 -0.059829 -0.073888   \n",
              "\n",
              "         top_con   mid_con   bot_con  basic_sim  \n",
              "0      -0.201026 -0.274885 -0.276059   0.530072  \n",
              "1      -0.127436  0.029195  0.115974   0.124346  \n",
              "2       0.025626 -0.037318 -0.163204   0.631895  \n",
              "3      -0.059336 -0.135955 -0.267575   0.585037  \n",
              "4      -0.165122 -0.145344 -0.157995   0.191970  \n",
              "...          ...       ...       ...        ...  \n",
              "416763 -0.141144 -0.208176 -0.301036   0.453219  \n",
              "416764 -0.172440 -0.069362 -0.090332   0.495821  \n",
              "416765  0.053380  0.066035  0.089771   0.313336  \n",
              "416766  0.165119  0.064642 -0.052814   0.689448  \n",
              "416767 -0.034677 -0.087364 -0.108276  -0.107607  \n",
              "\n",
              "[416768 rows x 41 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>original_text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_lemma</th>\n",
              "      <th>text_lemma_no_punc</th>\n",
              "      <th>basic_rank</th>\n",
              "      <th>basic_rank_no_punc</th>\n",
              "      <th>oov_rank</th>\n",
              "      <th>oov_rank_no_punc</th>\n",
              "      <th>word_len</th>\n",
              "      <th>...</th>\n",
              "      <th>aol_rank_no_punc_perc</th>\n",
              "      <th>stop_word</th>\n",
              "      <th>nums</th>\n",
              "      <th>top_aol</th>\n",
              "      <th>mid_aol</th>\n",
              "      <th>bot_aol</th>\n",
              "      <th>top_con</th>\n",
              "      <th>mid_con</th>\n",
              "      <th>bot_con</th>\n",
              "      <th>basic_sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>There is manuscript evidence that Austen conti...</td>\n",
              "      <td>1</td>\n",
              "      <td>there be manuscript evidence that austen conti...</td>\n",
              "      <td>there be manuscript evidence that austen conti...</td>\n",
              "      <td>0.348837</td>\n",
              "      <td>0.394737</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.394737</td>\n",
              "      <td>...</td>\n",
              "      <td>3.110604</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.394263</td>\n",
              "      <td>-0.380458</td>\n",
              "      <td>-0.248316</td>\n",
              "      <td>-0.201026</td>\n",
              "      <td>-0.274885</td>\n",
              "      <td>-0.276059</td>\n",
              "      <td>0.530072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>In a remarkable comparative analysis , Mandaea...</td>\n",
              "      <td>1</td>\n",
              "      <td>in a remarkable comparative analysis , mandaea...</td>\n",
              "      <td>in a remarkable comparative analysis mandaean ...</td>\n",
              "      <td>0.304348</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>6.190476</td>\n",
              "      <td>...</td>\n",
              "      <td>3.410769</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0.026090</td>\n",
              "      <td>-0.024771</td>\n",
              "      <td>-0.064591</td>\n",
              "      <td>-0.127436</td>\n",
              "      <td>0.029195</td>\n",
              "      <td>0.115974</td>\n",
              "      <td>0.124346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Before Persephone was released to Hermes , who...</td>\n",
              "      <td>1</td>\n",
              "      <td>before persephone be release to hermes , who h...</td>\n",
              "      <td>before persephone be release to hermes who hav...</td>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.952381</td>\n",
              "      <td>4.738095</td>\n",
              "      <td>...</td>\n",
              "      <td>3.921771</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.227885</td>\n",
              "      <td>-0.169121</td>\n",
              "      <td>0.024210</td>\n",
              "      <td>0.025626</td>\n",
              "      <td>-0.037318</td>\n",
              "      <td>-0.163204</td>\n",
              "      <td>0.631895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Cogeneration plants are commonly found in dist...</td>\n",
              "      <td>1</td>\n",
              "      <td>cogeneration plant be commonly find in distric...</td>\n",
              "      <td>cogeneration plant be commonly find in distric...</td>\n",
              "      <td>0.435897</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.281250</td>\n",
              "      <td>...</td>\n",
              "      <td>5.745808</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.316036</td>\n",
              "      <td>-0.298752</td>\n",
              "      <td>-0.165115</td>\n",
              "      <td>-0.059336</td>\n",
              "      <td>-0.135955</td>\n",
              "      <td>-0.267575</td>\n",
              "      <td>0.585037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Geneva -LRB- , ; , ; , ; ; -RRB- is the second...</td>\n",
              "      <td>1</td>\n",
              "      <td>geneva -lrb- , ; , ; , ; ; -rrb- be the second...</td>\n",
              "      <td>geneva -lrb- -rrb- be the second-most-populous...</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>5.642857</td>\n",
              "      <td>...</td>\n",
              "      <td>2.514700</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.206032</td>\n",
              "      <td>-0.194760</td>\n",
              "      <td>-0.154996</td>\n",
              "      <td>-0.165122</td>\n",
              "      <td>-0.145344</td>\n",
              "      <td>-0.157995</td>\n",
              "      <td>0.191970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416763</th>\n",
              "      <td>416763</td>\n",
              "      <td>A Duke Nukem 3D version has been sold for Xbox...</td>\n",
              "      <td>0</td>\n",
              "      <td>a duke nukem 3d version have be sell for xbox ...</td>\n",
              "      <td>a duke nukem 3d version have be sell for xbox ...</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.066667</td>\n",
              "      <td>...</td>\n",
              "      <td>1.926667</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.367020</td>\n",
              "      <td>-0.345096</td>\n",
              "      <td>-0.209537</td>\n",
              "      <td>-0.141144</td>\n",
              "      <td>-0.208176</td>\n",
              "      <td>-0.301036</td>\n",
              "      <td>0.453219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416764</th>\n",
              "      <td>416764</td>\n",
              "      <td>However , it is becoming replaced as a method ...</td>\n",
              "      <td>0</td>\n",
              "      <td>however , it be become replace a a method of e...</td>\n",
              "      <td>however it be become replace a a method of exe...</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.725556</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.191397</td>\n",
              "      <td>-0.221321</td>\n",
              "      <td>-0.186635</td>\n",
              "      <td>-0.172440</td>\n",
              "      <td>-0.069362</td>\n",
              "      <td>-0.090332</td>\n",
              "      <td>0.495821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416765</th>\n",
              "      <td>416765</td>\n",
              "      <td>There are hand gestures in both Hindu and Budd...</td>\n",
              "      <td>0</td>\n",
              "      <td>there be hand gesture in both hindu and buddhi...</td>\n",
              "      <td>there be hand gesture in both hindu and buddhi...</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.300000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.422300</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.012151</td>\n",
              "      <td>0.023066</td>\n",
              "      <td>0.073097</td>\n",
              "      <td>0.053380</td>\n",
              "      <td>0.066035</td>\n",
              "      <td>0.089771</td>\n",
              "      <td>0.313336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416766</th>\n",
              "      <td>416766</td>\n",
              "      <td>If it is necessary to use colors , try to choo...</td>\n",
              "      <td>0</td>\n",
              "      <td>if it be necessary to use color , try to choos...</td>\n",
              "      <td>if it be necessary to use color try to choose ...</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>0.487179</td>\n",
              "      <td>0.952381</td>\n",
              "      <td>0.948718</td>\n",
              "      <td>4.410256</td>\n",
              "      <td>...</td>\n",
              "      <td>4.231075</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.128926</td>\n",
              "      <td>-0.091671</td>\n",
              "      <td>0.108809</td>\n",
              "      <td>0.165119</td>\n",
              "      <td>0.064642</td>\n",
              "      <td>-0.052814</td>\n",
              "      <td>0.689448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416767</th>\n",
              "      <td>416767</td>\n",
              "      <td>Calgary Stampeders ,</td>\n",
              "      <td>0</td>\n",
              "      <td>calgary stampeders ,</td>\n",
              "      <td>calgary stampeders</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>8.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.072140</td>\n",
              "      <td>-0.059829</td>\n",
              "      <td>-0.073888</td>\n",
              "      <td>-0.034677</td>\n",
              "      <td>-0.087364</td>\n",
              "      <td>-0.108276</td>\n",
              "      <td>-0.107607</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>416768 rows × 41 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F1smmwKTp5yZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}